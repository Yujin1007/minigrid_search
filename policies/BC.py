from imitation.algorithms.bc import BC
from imitation.algorithms.bc import BCLogger, BehaviorCloningLossCalculator
from typing import (
    Any,
    Callable,
    Dict,
    Iterable,
    Iterator,
    Mapping,
    Optional,
    Tuple,
    Type,
    Union,
)
import torch
import gymnasium as gym
import numpy as np
import torch as th
import tqdm
from stable_baselines3.common import policies, torch_layers, utils, vec_env

from imitation.algorithms import base as algo_base
from imitation.data import rollout, types
from imitation.policies import base as policy_base
from imitation.util import logger as imit_logger
from imitation.util import util

#
# class customBC(BC):
#     def __init__(
#         self,
#         *,
#         observation_space: gym.Space,
#         action_space: gym.Space,
#         rng: np.random.Generator,
#         policy: Optional[policies.ActorCriticPolicy] = None,
#         demonstrations: Optional[algo_base.AnyTransitions] = None,
#         batch_size: int = 32,
#         minibatch_size: Optional[int] = None,
#         optimizer_cls: Type[th.optim.Optimizer] = th.optim.Adam,
#         optimizer_kwargs: Optional[Mapping[str, Any]] = None,
#         ent_weight: float = 1e-3,
#         l2_weight: float = 0.0,
#         device: Union[str, th.device] = "auto",
#         custom_logger: Optional[imit_logger.HierarchicalLogger] = None,
#         net_arch=[128, 128],
#     ):
#         self._demo_data_loader: Optional[Iterable[types.TransitionMapping]] = None
#         self.batch_size = batch_size
#         self.minibatch_size = minibatch_size or batch_size
#         if self.batch_size % self.minibatch_size != 0:
#             raise ValueError("Batch size must be a multiple of minibatch size.")
#         super().__init__(
#             demonstrations=demonstrations,
#             custom_logger=custom_logger,
#         )
#         self._bc_logger = BCLogger(self.logger)
#
#         self.action_space = action_space
#         self.observation_space = observation_space
#
#         self.rng = rng
#
#         if policy is None:
#             extractor = (
#                 torch_layers.CombinedExtractor
#                 if isinstance(observation_space, gym.spaces.Dict)
#                 else torch_layers.FlattenExtractor
#             )
#             policy = FeedForwardPolicy(
#                 observation_space=observation_space,
#                 action_space=action_space,
#                 lr_schedule=lambda _: th.finfo(th.float32).max,
#                 features_extractor_class=extractor,
#                 net_arch=net_arch
#             )
#         self._policy = policy.to(utils.get_device(device))
#         # TODO(adam): make policy mandatory and delete observation/action space params?
#         assert self.policy.observation_space == self.observation_space
#         assert self.policy.action_space == self.action_space
#
#         if optimizer_kwargs:
#             if "weight_decay" in optimizer_kwargs:
#                 raise ValueError("Use the parameter l2_weight instead of weight_decay.")
#         optimizer_kwargs = optimizer_kwargs or {}
#         self.optimizer = optimizer_cls(
#             self.policy.parameters(),
#             **optimizer_kwargs,
#         )
#
#         self.loss_calculator = BehaviorCloningLossCalculator(ent_weight, l2_weight)
#

class FeedForwardPolicy(policy_base.FeedForward32Policy):
    """A feed forward policy network with two hidden layers of 32 units.

    This matches the IRL policies in the original AIRL paper.

    Note: This differs from stable_baselines3 ActorCriticPolicy in two ways: by
    having 32 rather than 64 units, and by having policy and value networks
    share weights except at the final layer, where there are different linear heads.
    """

    def __init__(self, net_arch, *args, **kwargs):
        """Builds FeedForward32Policy; arguments passed to `ActorCriticPolicy`."""
        super().__init__(*args, **kwargs, net_arch=net_arch)

class BCCustom(BC):
    def get_action_probabilities(self, observation):
        obs_tensor = torch.tensor(observation, dtype=torch.float32).unsqueeze(0)
        # logits = self.policy(obs_tensor)
        features = self.policy.mlp_extractor.policy_net(self.policy.features_extractor(obs_tensor))
        action_logits = self.policy.action_net(features)
        probabilities = torch.softmax(action_logits, dim=-1).detach().numpy()
        return probabilities